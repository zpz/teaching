\documentclass[12pt]{article}
\usepackage{coursenote}
\begin{document}
\title{STAT 401 Chapter 3.1--3.4, 3.8, 3.9}
\maketitle

While we can always fit a simple linear model (SLM) to a
$Y \sim X$ data set, and the calculation is easy,
this model may not be appropriate for the data set
(and the subject behind the data).
The ``inappropriateness'' is usually in the form of violation  to
one or more \emph{assumptions} of the model:
\begin{enumerate}
\item \[E(Y \given X) = \beta_0 + \beta_1 X\]
    that is, between $E(Y)$ and $X$ is a \emph{linear relationship}.
\item \[ \epsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2) \]
    that is,
    \begin{enumerate}
        \item $\epsilon$'s at different $X$ levels are independent.
        \item $\var(\epsilon) = \sigma^2$, constant, does not change
            with $X$.
        \item $\epsilon$ is normal.
    \end{enumerate}
\end{enumerate}
In addition, an important problem to check for is
\begin{enumerate}
\item[3.] There are important predictors other than $X$ that should have
been included in modeling $Y$.
\end{enumerate}

\bigskip

The most efficient diagnostics are graphics, mainly (1) scatter plot of
$Y \sim X$; (2) various plots of the residuals $e_i$.

Graphical methods are \emph{informal}, but are really effective and
simple.
They are usually adequate in discovering problems,
although you may need to (or want to) provide something more formal
(fancier) to support that a problem that is obvious in the graphics
does exist.

``Formal methods'' refer to various statistical \emph{tests}.
They are all based on the same idea:
find a test statistic whose sampling distribution is known
\emph{provided a certain assumption (e.g.\@ $\epsilon$ is normal) holds},
then if the observed value of the statistic is an extreme value
in its sampling distribution, it suggests the assumption in question
most likely does not hold.

The various problems in a model are intertwined.
For example,
problem A may obscure problem B,
or make an nonexistent problem B appear serious.
One fix often alleviates more than one problem.


\section{Basic checks on the data}

Watch for the following problems in $X$:
\begin{enumerate}
\item Values of $X$ are not distributed in a balanced fashion
    in the observed range. Use a dot plot.
\item Outlying values of $X$. Use a dot plot or box plot.
\end{enumerate}

Watch for outlying $(x,y)$ data points.
Such points often have both $x$ and $y$ perfectly within
their respective ranges but the point (the combination of $x$ and $y$
values) is unusual compared to other data points.
\begin{enumerate}
\item $Y \sim X$ scatter plot.
\item $e_i \sim x_i$ plot.
\end{enumerate}

A outlying data point (either $x$ is outlying or the $(x,y)$ combination
is outlying) is overly influential on the model estimates.
Outlying observations must be treated somehow,
but should not be discarded
unless there is evidence that the values are erroneous.


\section{Is a \emph{linear} model suitable?}

\begin{enumerate}
\item Look at a $Y \sim X$ scatter plot. Fig~3.3(a), page 105.
\item Plot $e_i$ against $x_i$. Symptom: $e_i$ does not appear to
fluctuate up and down randomly, but show some pattern of undulation.
Fig~3.3(b), page 105. Fig~3.4(b), page 106.

\emph{Caution}: an outlier point may cause this effect as well.
Fig~3.7, page 109.
\end{enumerate}

Method 1 can be used before model fitting is conducted.
By method~2 the problem may be more pronounced.
Usually method~1 should be adequate for this problem.

Fixes:\\
1. Use a nonlinear model. (Not covered.)\\
2. Transform $X$. (In a moment.)

Note: when the model is inappropriate,
subsequent inferences (Chapter 2) become meaningless.
See comment~4, page 127.

\section{Diagnostics with residuals}

\[
e_i = Y_i - \hat{Y}_i = Y_i - \hat\beta_0 - \hat\beta_1 X_i
\]

The residuals are somewhat like estimates of the random error term
($\epsilon$) in the model, but not quite.

Note that $e_i$ is a random variable if we repeat the data ($Y$, that is)
sampling and model fitting procedure many times.

\textbf{Properties of the residuals} (p. 23--24, 102--103):

\indentblock{%
1. $e_i$ has a normal distribution with mean 0,
because $e_i$ is a linear combination of normal variables $Y_i$,
$\hat\beta_0$, $\hat\beta_1$, and $E(e_i) = E(Y_i) - E(\hat{Y}_i) = 0$.

2. $\var(e_i)$ is \emph{not} constant; it varies with $X_i$.
If we work out $\var(e_i)$, we'll see it depends on $X_i$.
(The farther away from the center of $X$, the larger the $\var(e_i)$.)

3. $\sum e_i = 0$. (Proven earlier.)

4. $\sum X_i e_i = 0$.

5. The $e_i$'s are \emph{not independent}.
This is apparent from the constraints 3 and~4.
(They can't be independent, since they have to ``cooperate'' to satisfy
these relations.)
If we work out $\cov(e_i, e_j)$, we'll see it's nonzero,
which also suggests $e_i$ and $e_j$ are dependent.

6. We call $s^2$ (or MSE) the ``variance'' of the residuals. Recall
$s^2 = \frac{\sum e_i^2}{n-2} = \frac{\sum (e_i -
\overline{e})^2}{n-2}$.
Remember $s^2$ is an unbiased estimator of $\sigma^2$.
However, since the $e_i$'s are not a random sample from a common
distribution, it is a little unclear what we mean by the ``variance'' of
the residuals. (They do not have the same variance, according to
property~2.)
}

The properties 1, 2, and 5 will become trivial after we learn multiple
regression and use matrix.

When the sample size ($n$) is reasonably large (and the $X$ values are
spread out ``nicely''), the residuals are a very effective diagnostic
tool---they help to identify violations of the assumptions about
$\epsilon$.

\textbf{Semistudentized residuals} (page 103)

\[
e_i^* = \frac{e_i - \overline{e}}{\sqrt{s^2}} = \frac{e_i}{s}
\]

This is analogous to ``standardizing'' a normal variable to standard
normal. The $e_i^*$ has approximately a $t_{n-2}$ distribution
(which is not very different from the standard normal).
Because the $t$ distribution is also called ``student $t$''
distribution, this operation is called ``studentization''.
In addition, because the statement $e_i^* \sim t_{n-2}$ is only
approximate here, we add ``semi-'' in front of ``studentization''.



\subsection{Nonconstancy of error variance}

A $e_i \sim x_i$ plot. Fig~3.4(c), page 106. Fig~3.5, page 107.

$Y \sim X$ may also suggest this problem, but less clearly than
a $e_i \sim x_i$ plot.

A $e_i \sim \hat{Y}_i$ plot can spot the same problem.

Fix: often calls for a transformation on $Y$.
This operation is called ``variance stabilization'.


\subsection{Nonindependence of error terms}

This is a concern primarily when there is an ordering of the data, for
example by time or spatial location.

Plot $e_i$ against the index variable (say time or spatial coordinates).
If the residuals show a pattern of undulation,
it indicates dependence between the error terms.
Fig~3.8, page 109.

Note: such patterns are not visible on a $e_i \sim x_i$ plot
(unless $X$ itself is the ordering index variable),
because the order of $X$ values is different from the order of
time/space.

Fix: Do something about the index variable.
See Chapter~12 (not covered in this course).

\subsection{Nonnormality of error terms}

To check normality of a sample,
here $\{e_i\}$, the most commonly used tool is a \emph{normal
probability plot}.

Suppose the residuals are $e_1, e_2, \dotsc, e_n$, ordered from small to
large. Very roughly speaking, $e_i$ is the $\frac{i}{n}$ quantile of
the distribution behind this sample.

However, this is not quite right.
For example, is $e_n$ the $\frac{n}{n}$ quantile?
No! That would mean $e_n$ is the maximum possible value in the
distribution---we have no reason to believe this.
So, some tuning (with theoretical justifications) is needed.
One tuning is this:
\begin{multline*}
\text{regard $e_i$ as (an estimate of) the}\\
    \frac{i-.375}{n+.25} \text{ quantile of the distribution of $e$}
\end{multline*}

(An alternative tuning:
regard $e_i$ as the $\frac{i-.5}{n}$ quantile.)

(Recall that the $q$ quantile, written as $x_q$ for example,
of a distribution $F(x)$ is such that $F(x_q) = q$,
where $F(x)$ is the cdf.)

The normal probability plot is a plot of
the $\frac{i-.375}{n+.25}$, $i=1,\dotsc,n$, quantiles
of $N(0,1)$ against $e_1,\dotsc,e_n$.
That is,
it's a plot of corresponding quantiles of the distribution of $e$ and of
a standard normal distribution.
For this reason, it's called a QQ plot.

\textbf{If the plot shows (roughly) a straight line,
then the distribution of the residuals is \emph{normal};
if the straight line is $Y = X$, then the distribution of the residuals
is \emph{standard normal}.}

Note: It's a general straight line; it does not need to go through
$(0,0)$, and its slope does not need to be 1.

If $e$ is non-normal, the usual pattern is the two ends
of the plot deviate from a straight line.
From the direction of the deviations,
one can tell how the tails of the distribution of $e$
differ from a normal distribution.

\example
Say something about the deviation from normal by looking at
the normal probability plot.
Fig~3.9, page 112.

\alert[\texttt{R} functions]%
\texttt{qqnorm}, \texttt{qqline}, \texttt{qqplot}.

\alert
\begin{enumerate}
\item Normal probability plot is available in possibly all statistical
    computer software. It's a special case of Q-Q plot.
    The general Q-Q plot would plot corresponding quantiles of two
    distributions, or two datasets, or a dataset and a known
    distribution, in order to compare the distributions.
\item The normality assumption is that $\epsilon$ corresponding to
    any fixed $x$ is normal.
    Here, $e$ is treated as estimates of $\epsilon$.
    But $e_i$ correspond to different $X$ values.
    We actually are assuming independence and constant variance for $e$,
    and are pooling the $e$'s at different $X$ and treating them
    as something like an iid sample of $\epsilon$ (whose distribution is
    the same regardless of the value of $X$).

    For this reason,
    \emph{independence and constant variance should be checked before normality}.
    Only after the former two checks pass does it make sense to check
    normality.

\item
Why a straight line?

(1) If two distributions have identical CDF's, then the distributions
are the same.

(2) If corresponding quantiles are all equal, then the two distributions
have the same CDF's (hence the distributions are the same).

(3) If quantiles of distribution~1 are all equal to those of
distribution~2 \emph{after a linear transform}, then the linear
transform of distribution~1 is the same as distribution~2.

(4) A linear transform of a normal distribution is normal.

Therefore,
if the linear transform of $e$ has a standard normal distribution,
then $e$ has a (non-standard) normal distribution.


% Let $X \sim N(\mu, \sigma^2)$ and $Z \sim N(0,1)$.
% Write the $q$ quantiles of $X$ and $Z$ as
% $x_q$ and $z_q$, respectively.
% Then,
% \[\begin{split}
% P(X \le x_q) &= q
% \\
% \Leftrightarrow\quad
% P\Bigl(\frac{X - \mu}{\sigma} \le \frac{x_q - \mu}{\sigma}\Bigr) &= q
% \\
% \Leftrightarrow\quad
% P\Bigl(Z \le \frac{x_q - \mu}{\sigma}\Bigr) &= q
% \\
% \Leftrightarrow\quad
% \frac{x_q - \mu}{\sigma} &= z_q
% \end{split}\]
% Therefore there is a linear relationship between
% $x_q$ and $z_q$.
\end{enumerate}

\subsection{Omission of important predictor variables}

Suppose we've estimated model $Y = \beta_0 + \beta_1 X + \epsilon$.
There is another variable, $S$, that possibly is closely related to
$Y$. To check whether we should include $S$ in the model,
plot residuals $e_i$ against $s_i$.
If some pattern appears, then introducing $S$ may ``explain'' this
pattern.

$S$ may well be categorical.
For example, $S$ represent two geographical locations.
Do the $e$'s at one location differ systematically from
those $e$'s at the other location?

Fix: see Chapter~8.

\example
Fig~3.10, page 113.


\section{Overview of remedial measures}

\begin{enumerate}
\item Abandon linear regression, develop a more appropriate model.

\item Add predictors (``multiple'', rather than ``simple'', linear
regression).

\item Transform $X$ or $Y$ or both.
\end{enumerate}


\section{Transformations}

The idea is to fit a SLR with transforms of $X$ and/or $Y$.
Interpretation of the model is then in terms of the transformed variables.
Hypothesis tests, and confidence intervals are completed
on the transformed variables.
\emph{The purpose of such transformations is to meet the
basic assumptions of the model}.
(Remember, validity of the estimation and inference methods are all
contingent upon the validity of the model assumptions.)
There are two basic reasons to
transform:

\begin{enumerate}
\item
To linearize a relationship.

\item
To achieve normality and constant variance  of errors.
(Often, fixing
one of these problems fixes the other as well.)
\end{enumerate}


\begin{center}
\begin{tabular}{cllll}
case & const & linear & possible fix & examples
\\
& err var? & relation? & &
\\ \hline
1 & yes  & no & transform $X$; don't transform $Y$ & Fig.~3.13, page 130
\\
2 & no & no & transform $Y$ & Fig.~3.15, page 132
\\
3 & no & yes & transform $Y$, then (possibly) $X$ &
\\ \hline
\end{tabular}
\end{center}

In case 1, how do we find the ``right'' transformation?---%
If the curve looks like that of $f(x)$ (say $x^2$, $\log(x)$, etc),
then try replacing $X$ by $f(X)$.

In case 2, a nonlinear transform of $Y$ ``compresses'' and ``stretches''
large and small $Y$'s differently, hence may remedy the uneven spread of
$Y$ (\ie non-constant error variance).
Transforming $Y$ will change the curve---a nonlinear relation could
become linear, in which case transforming $X$ is not needed.

In case 3, the important thing is to transform $Y$ first so as to
stabilize the error variance.
This will change the curve, and a linear relation may become nonlinear.
Then we're in case~1, so try a transform on $X$ next.

\bigskip
Non-normality of the residuals may be fixed as we fix the non-constant
variance problem. We (usually) do not attack non-normality directly.

\bigskip
Drawback of transforming data ($X$ or $Y$): It becomes harder to interpret
your results (in terms of the untransformed variables).

\bigskip
\textbf{Box-Cox transformations}:
this is a widely used ``family'' of transforms.
It tries to fix several possible problems at once.
However it has its own problems.
You should know about this tool and recognize its formula.

% Box-Cox is a class of power transformations with one parameter, $\lambda$.
% Its common (and original) formula is
% \[
% Y(\lambda) = \frac{Y^\lambda - 1}{\lambda}
% \]
% This contains several familiar special cases.
% In particular, $Y(0) = \log Y$
% (by taking limits).
% 
% The book takes
% \[
% Y(\lambda) = Y^\lambda
% \]
% which makes some forms simpler;
% but then $\lambda = 0$ no longer gives the log transform,
% but rather is \emph{defined} to be the log transform.
% 
% The motivation for introducing this transformation
% is to make the linear model assumptions hold
% on the transformed variable,
% hence linear regression can proceed.
% This is sometimes, but not always, successful.
% 
% The general procedure for determining $\lambda$ is described in the
% book.
% 
% In practice, we use the Box-Cox procedure to get an approximate value for $\lambda$,
% at which point we use a nearby rounded value for $\lambda$.  For example, if the
% suggested $\lambda$ is greater than 1, round $\lambda$ to the nearest integer.
% If the suggested $\lambda$ is between -1 and +1, round $\lambda$ to the nearest 1/2.
% If $\lambda$ is between -1/2 and +1/2, then use $\lambda = 0$.
% (It is beyond interpretation, and essentially useless,
% to use an arbitrary number obtained through an optimization algorithm,
% e.g.\@ $\lambda = -0.67$.)
% 
% (You should know about the Box-Cox transformation.
% Systematic use of it is not required.)


\end{document}
